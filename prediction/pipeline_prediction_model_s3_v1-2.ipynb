{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986b2274-ae2c-4555-bb8d-9d950f9869a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from typing import NamedTuple\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, component, Dataset)\n",
    "from google.cloud import storage\n",
    "from google_cloud_pipeline_components.v1.vertex_notification_email import VertexNotificationEmailOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef85c80-40a6-48c1-906c-f657cdfcb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def error_op(msg: str):\n",
    "    raise(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "559afd52-909e-4c38-a4a6-676759b9b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-bigquery\"])\n",
    "def validate_data(\n",
    "    source_x_train_table: str,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"condition\", str)\n",
    "    ],\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    try:\n",
    "        client.get_table(source_x_train_table)\n",
    "        condition = \"true\"\n",
    "    except Exception as e:\n",
    "        condition = \"false\"\n",
    "\n",
    "    return (condition,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e57077-1655-4f4e-9356-4f506944b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-bigquery-storage\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"joblib\",\n",
    "        \"db-dtypes\",\n",
    "        \"pyarrow\",\n",
    "        \"pandas-gbq\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"pytz\"\n",
    "    ],\n",
    ")\n",
    "def prediction(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    table_id: str,\n",
    "    path_model: str,\n",
    "):  \n",
    "    import sys\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    from google.auth import default\n",
    "    import pandas_gbq\n",
    "    from google.cloud import storage\n",
    "    from joblib import load\n",
    "    from io import BytesIO\n",
    "    from pytz import timezone\n",
    "    \n",
    "    TZ = timezone('America/Lima')\n",
    "    FORMAT_DATE = \"%Y-%m-%d\"\n",
    "    \n",
    "    # Cliente BigQuery\n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "    # Leer datos de BigQuery\n",
    "    X_train = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=source_x_train_table)).to_dataframe()\n",
    "\n",
    "    \n",
    "    # Leer características seleccionadas de BigQuery\n",
    "    features = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=features_table)).to_dataframe()\n",
    "    \n",
    "    features = features[\"string_field_0\"].tolist()\n",
    "    \n",
    "\n",
    "    X_train = X_train[features]\n",
    "    \n",
    "    def generate_datetime_created():\n",
    "        return datetime.now()\n",
    "    \n",
    "    def generate_date_created():\n",
    "        return datetime.now(TZ).date().strftime(FORMAT_DATE)\n",
    "    \n",
    "    \n",
    "    def load_model_from_gcs(path_model):\n",
    "        # Inicializar el cliente de Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Obtener el nombre del bucket y la ruta del objeto\n",
    "        bucket_name, blob_name = path_model.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "\n",
    "        # Obtener el objeto desde Cloud Storage\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        model_bytes = blob.download_as_string()\n",
    "\n",
    "        # Cargar el modelo desde los bytes obtenidos\n",
    "        classifier = load(BytesIO(model_bytes))\n",
    "\n",
    "        return classifier\n",
    "\n",
    "    classifier = load_model_from_gcs(path_model)\n",
    "\n",
    "    # Realizar la predicción\n",
    "    predictions = classifier.predict(X_train)\n",
    "    predictions = pd.DataFrame(predictions, columns=['prediction'])\n",
    "\n",
    "    # Obtener el user_id de la sesión actual en BigQuery\n",
    "    user_id = client.query(\"SELECT SESSION_USER()\").to_dataframe().iloc[0, 0]\n",
    "\n",
    "    # Agregar campos de auditoría\n",
    "    start_time = generate_datetime_created()\n",
    "    execute_date = generate_date_created()\n",
    "    \n",
    "    predictions['creation_user'] = user_id\n",
    "    predictions['process_date'] = datetime.strptime(execute_date, '%Y-%m-%d')\n",
    "    predictions['process_date'] = pd.to_datetime(predictions['process_date']).dt.date\n",
    "    predictions['load_date'] = pd.to_datetime(start_time)\n",
    "    \n",
    "    # Guardar el resultado en BigQuery \n",
    "    pandas_gbq.to_gbq(predictions , table_id, if_exists='append', project_id=project)\n",
    "\n",
    "    print(\"Predicción generada y guardada en BigQuery.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22d6605f-8ca6-4287-bc46-7ad62a8cabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"intro\", \n",
    "    description=\"intro\",\n",
    "    pipeline_root=\"gs://diabetes-bucket1/proyectoD\"\n",
    ")\n",
    "\n",
    "def main_pipeline(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    table_id: str,\n",
    "    path_model: str,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "):\n",
    "    \n",
    "    notify_email_task = VertexNotificationEmailOp(recipients=[\"sojuanesi4@gmail.com\"])\n",
    "    notify_email_task.set_display_name('Notification Email')\n",
    "    \n",
    "    with dsl.ExitHandler(notify_email_task, name=\"Execute pipeline prediction\"):\n",
    "\n",
    "        validate_tables_job = validate_data(\n",
    "            source_x_train_table = source_x_train_table\n",
    "        )\n",
    "        validate_tables_job.set_display_name('Validate Data')\n",
    "\n",
    "        with dsl.Condition(\n",
    "            validate_tables_job.outputs['condition']==\"false\",\n",
    "            name=\"no-execute\",\n",
    "        ):\n",
    "            error_op(\"No se logro validar las tablas de ingesta.\")\n",
    "\n",
    "\n",
    "        with dsl.Condition(\n",
    "            validate_tables_job.outputs['condition']==\"true\",\n",
    "            name=\"execute\",\n",
    "        ):\n",
    "  \n",
    "            prediction_tast = prediction(\n",
    "                project = project,\n",
    "                source_x_train_table = source_x_train_table,\n",
    "                features_table = features_table,\n",
    "                table_id = table_id,\n",
    "                path_model = path_model,\n",
    "            )\n",
    "            prediction_tast.set_display_name(\"PREDICTION_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a09581f-2348-45b5-8533-225e3d04d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=main_pipeline,\n",
    "    package_path=\"pipeline_prediction.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dd4e0a1-9ca9-4a10-964d-a2a9469af54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo pipeline_prediction.json subido a demo/pipeline_prediction.json en el bucket dev-dp-vertex-analytics-ai.\n"
     ]
    }
   ],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"Archivo {source_file_name} subido a {destination_blob_name} en el bucket {bucket_name}.\")\n",
    "\n",
    "# Define las variables\n",
    "bucket_name = \" diabetes-bucket1\"\n",
    "destination_blob_name = \"demo/pipeline_prediction.json\"\n",
    "pipeline_file = \"pipeline_prediction.json\"\n",
    "# Llamar a la función para subir el archivo\n",
    "upload_to_gcs(bucket_name, pipeline_file, destination_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d30531d7-f798-420e-9cde-d8bd781deb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=\"diabetes-datapath1\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "717e7da3-41a5-4a91-b763-a36acfdc6c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit pipeline job ...\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/624205664083/locations/us-central1/pipelineJobs/intro-20240624043003\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/624205664083/locations/us-central1/pipelineJobs/intro-20240624043003')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/intro-20240624043003?project=624205664083\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline de prueba\",\n",
    "    template_path=\"gs://diabetes-bucket1/proyectoD/pipeline_prediction.json\",\n",
    "    #job_id=\"mlops72\",\n",
    "    enable_caching=False,\n",
    "    project=\"diabetes-datapath1\",\n",
    "    location=\"us-central1\",\n",
    "    parameter_values={\"project\": \"diabetes-datapath1\", \n",
    "                      \"source_x_train_table\": \"diabetes-datapath1.diabetes.xtrain\",\n",
    "                      \"features_table\": \"diabetes-datapath1.diabetes.selected_features\",\n",
    "                      \"table_id\": \"diabetes-datapath1.diabetes.predictions\",\n",
    "                      \"path_model\": \"gs://diabetes-bucket1/proyectoD/data/model/model.joblib\"\n",
    "                     }\n",
    "    #labels={\"module\": \"ml\", \"application\": \"app\", \"chapter\": \"mlops\", \"company\": \"datapat\", \"environment\": \"dev\", \"owner\": \"xxxx\"}\n",
    ")\n",
    "\n",
    "print('submit pipeline job ...')\n",
    "job.submit(service_account=\"diabetes@diabetes-datapath1.iam.gserviceaccount.com\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
