{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c97b043-edcd-46ab-93f5-7f159ca10cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el CSV\n",
    "df = pd.read_csv('data/y_train/ytrain.csv')\n",
    "\n",
    "df.to_csv('data/pipeline_train_model_v1-1/ytrain.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d632b831-1c9f-4105-b633-b5de49b7a7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 466f79a0-1f62-47a4-b45c-a20aa06b6993 for ytrain.csv\n",
      "Job 466f79a0-1f62-47a4-b45c-a20aa06b6993 finished for ytrain.csv\n",
      "Loaded 2340 rows into diabetes-datapath1.diabetes.ytrain.\n",
      "Starting job 93b18b6b-8478-4ca3-8675-8838854d67f5 for selected_features.csv\n",
      "Job 93b18b6b-8478-4ca3-8675-8838854d67f5 finished for selected_features.csv\n",
      "Loaded 50 rows into diabetes-datapath1.diabetes.selected_features.\n",
      "Starting job 09a5dd87-76c6-46db-92a8-65f355726e97 for xtrain.csv\n",
      "Job 09a5dd87-76c6-46db-92a8-65f355726e97 finished for xtrain.csv\n",
      "Loaded 2340 rows into diabetes-datapath1.diabetes.xtrain.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "# Cliente BigQuery\n",
    "client = bigquery.Client()\n",
    "\n",
    "project_id = \"diabetes-datapath1\"\n",
    "dataset_id = \"diabetes\"\n",
    "folder_path = \"data/pipeline_train_model_v1-1\"\n",
    "\n",
    "# Configurar la tabla de destino\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "# Iterar sobre los archivos en la carpeta\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        table_id = os.path.splitext(file_name)[0] \n",
    "        table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "        # Configurar el job de carga\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "            skip_leading_rows=1,\n",
    "            autodetect=True,\n",
    "        )\n",
    "\n",
    "        csv_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Leer el archivo CSV y cargarlo a BigQuery\n",
    "        with open(csv_file_path, \"rb\") as source_file:\n",
    "            load_job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "        print(f\"Starting job {load_job.job_id} for {file_name}\")\n",
    "        load_job.result()  # Esperar a que termine el trabajo\n",
    "        print(f\"Job {load_job.job_id} finished for {file_name}\")\n",
    "\n",
    "        destination_table = client.get_table(table_ref)\n",
    "        print(f\"Loaded {destination_table.num_rows} rows into {table_ref}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7bdd71-c11a-49a6-b372-9ee5613f6a82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, component,Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e57077-1655-4f4e-9356-4f506944b29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-bigquery-storage\",\n",
    "        \"pandas\",\n",
    "        \"db-dtypes\",\n",
    "        \"pyarrow\"\n",
    "    ],\n",
    ")\n",
    "def process_data(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    dataset: Output[Dataset],\n",
    "):\n",
    "    \n",
    "    import sys\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "    \n",
    "    X_train = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=source_x_train_table)).to_dataframe()\n",
    "    \n",
    "    features = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=features_table))\n",
    "    \n",
    "    df = features.to_dataframe()\n",
    "    \n",
    "    # Seleccionar características\n",
    "    features = df[\"string_field_0\"].tolist()\n",
    "\n",
    "    X_train = X_train[features]\n",
    "\n",
    "    X_train.to_parquet(f'{dataset.path}.parquet',engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a987ad19-6a8f-4c8f-a0fa-12568b03ca58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-bigquery-storage\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"joblib\",\n",
    "        \"db-dtypes\"\n",
    "    ],\n",
    ")\n",
    "def train_model(\n",
    "    project: str,\n",
    "    source_y_train_table: str,\n",
    "    inputd: Input[Dataset],\n",
    "):\n",
    "    \n",
    "    import sys\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import joblib\n",
    "\n",
    "    # Crear cliente de BigQuery\n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "    # Consultar los datos de Y_train desde BigQuery\n",
    "    Y_train = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=source_y_train_table)).to_dataframe()\n",
    "\n",
    "    # Leer datos de Parquet del conjunto de datos de entrada (X_train)\n",
    "    X_train = pd.read_parquet(f'{inputd.path}.parquet')\n",
    "    \n",
    "    # Configurar el modelo de regresión logística\n",
    "    log_model = LogisticRegression(random_state=0, max_iter=1000)  \n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    log_model.fit(X_train, Y_train) \n",
    "    \n",
    "    # Guardar el modelo entrenado en un archivo\n",
    "    model_filename = 'lasso_model.joblib'\n",
    "    joblib.dump(log_model, model_filename)\n",
    "    \n",
    "    # Subir el modelo a Google Cloud Storage (GCS)\n",
    "    storage_client = storage.Client(project=project)\n",
    "    bucket_name = \"diabetes-bucket1\"\n",
    "    destination_blob_name = \"proyectoD/data/model/model.joblib\"\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d6605f-8ca6-4287-bc46-7ad62a8cabd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"pipeline-training-model\", \n",
    "    description=\"intro\",\n",
    "    pipeline_root=\"gs://diabetes-bucket1/proyectoD\"\n",
    ")\n",
    "def main_pipeline(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    source_y_train_table: str,\n",
    "    features_table: str,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "):\n",
    "    get_data = process_data(\n",
    "        project = project,\n",
    "        source_x_train_table = source_x_train_table,\n",
    "        features_table = features_table\n",
    "    )\n",
    "    get_data.set_display_name(\"PROCESS_DATA\")\n",
    "    \n",
    "    train = train_model(\n",
    "        project = project,\n",
    "        source_y_train_table = source_y_train_table,\n",
    "        inputd = get_data.output\n",
    "    ).after(get_data)\n",
    "    train.set_display_name(\"TRAIN_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a09581f-2348-45b5-8533-225e3d04d95e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dev/lib/python3.8/site-packages/kfp/v2/compiler/compiler.py:1290: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=main_pipeline,\n",
    "    package_path=\"pipeline_training.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d30531d7-f798-420e-9cde-d8bd781deb10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=\"diabetes-datapath\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "717e7da3-41a5-4a91-b763-a36acfdc6c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit pipeline job ...\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/830795959507/locations/us-central1/pipelineJobs/pipeline-training-model-20241017233125\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/830795959507/locations/us-central1/pipelineJobs/pipeline-training-model-20241017233125')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipeline-training-model-20241017233125?project=830795959507\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline de prueba\",\n",
    "    template_path=\"pipeline_training.json\",\n",
    "    enable_caching=True,\n",
    "    project=\"diabetes-datapath1\",\n",
    "    location=\"us-central1\",\n",
    "    parameter_values={\"project\": \"diabetes-datapath1\", \n",
    "                      \"source_x_train_table\": \"diabetes-datapath1.diabetes.xtrain\",\n",
    "                      \"source_y_train_table\": \"diabetes-datapath1.diabetes.ytrain\",\n",
    "                      \"features_table\": \"diabetes-datapath1.diabetes.selected_features\"\n",
    "                     }\n",
    "    #labels={\"module\": \"ml\", \"application\": \"app\", \"chapter\": \"mlops\", \"company\": \"datapat\", \"environment\": \"dev\", \"owner\": \"xxxx\"}\n",
    ")\n",
    "\n",
    "print('submit pipeline job ...')\n",
    "job.submit(service_account=\"diabetes@diabetes-datapath1.iam.gserviceaccount.com\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "dev",
   "name": "common-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m125"
  },
  "kernelspec": {
   "display_name": "dev (Local)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
